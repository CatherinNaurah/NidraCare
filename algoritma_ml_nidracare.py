# -*- coding: utf-8 -*-
"""Algoritma ML NidraCare

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eACBm4oLH1Hmz9N_deGteOZ6OrRbDVFG

# Install Library
"""

!pip install kagglehub
!pip install tensorflowjs
!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install seaborn
!pip install tensorflow
!pip install scikit-learn

"""# Import Package"""

import kagglehub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""# Data Gathering"""

# Download dataset
path = kagglehub.dataset_download("uom190346a/sleep-health-and-lifestyle-dataset")
print("Path to dataset files:", path)

# Load dataset
df = pd.read_csv('/kaggle/input/sleep-health-and-lifestyle-dataset/Sleep_health_and_lifestyle_dataset.csv')

# Display initial data and information
print("Initial data preview:")
df.head()
# print(df.head())

print("\nData information:")
df.info()

"""# Data Assessing"""

# Check for duplicates
print('\nJumlah Data Duplikat : ', df.duplicated().sum())

# Check for missing values
print("\nMissing values per column:")
print(df.isnull().sum())

"""# Cleaning Data"""

df.drop(columns=['Person ID'], axis=1, inplace=True)

df[['Systolic', 'Diastolic']] = df['Blood Pressure'].str.extract(r'(\d+)/(\d+)').astype(int)
df

def jenisTekananDarah(systolic, diastolic):
    if systolic <= 120 and diastolic <= 80:
        return 'Normal'
    elif 120 < systolic <= 129 and diastolic <= 80:
        return 'Pra Hipertensi'
    elif (130 <= systolic <= 139) or (80 < diastolic < 89):
        return 'Hipertensi Tahap 1'
    else:
        return 'Hipertensi Tahap 2'

df['BP_Category'] = df.apply(lambda row: jenisTekananDarah(row['Systolic'], row['Diastolic']), axis=1)
df

df.drop(columns=['Blood Pressure', 'Systolic', 'Diastolic'], axis=1, inplace=True)

df['Sleep Disorder'] = np.where(df['Sleep Disorder'].isna(), 'Normal', df['Sleep Disorder'])
df.head()

# Display updated data
print("\nUpdated data sample:")
print(df.head())

gender_counts = df['Gender'].value_counts()
gender_percentages = gender_counts / len(df) * 100


plt.figure(figsize=(8, 8))
plt.pie(gender_percentages, labels=gender_percentages.index, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'pink'])
plt.title('Gender Distribution')
plt.axis('equal')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], bins=20, kde=True)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(12, 6))
ax = sns.countplot(x='Occupation', data=df, palette='viridis')
plt.title('Distribution of Occupation')
plt.xlabel('Occupation')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')


for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')

plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(df['Sleep Duration'], bins=10, kde=True)
plt.title('Distribution of Sleep Duration')
plt.xlabel('Sleep Duration (hours)')
plt.ylabel('Frequency')
plt.show()

# Grouping columns by data type
numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print("\nNumerical columns:")
print(numerical_cols)
print("\nCategorical columns:")
print(categorical_cols)

# Descriptive statistics for numerical variables
numeric_data = df.select_dtypes(include=['float64', 'int64'])
print("\nDescriptive statistics for numerical variables:")
print(numeric_data.describe())

"""# Data Preprocessing"""

# Data Encoding
from sklearn.preprocessing import LabelEncoder
gender_encoder = LabelEncoder()
occupation_encoder = LabelEncoder()
bmi_encoder = LabelEncoder()
bp_encoder = LabelEncoder()


df['Gender'] = gender_encoder.fit_transform(df['Gender'])
df['Occupation'] = occupation_encoder.fit_transform(df['Occupation'])
df['BMI Category'] = bmi_encoder.fit_transform(df['BMI Category'])
df['BP_Category'] = bp_encoder.fit_transform(df['BP_Category'])

df['Sleep Disorder'] = df['Sleep Disorder'].map({'Normal': 0, 'Sleep Apnea': 1, 'Insomnia': 2})

# Make sure data was encoded
print(df.head())

# Split Data Training and Target
x = df.drop('Sleep Disorder', axis=1)
y = df['Sleep Disorder']

# Check the results
print("x : ", x.shape)
print("y :", y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Normalization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert target to categorical format for Keras
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

"""# Modelling Classification"""

# Set random seed for reproducibility
import random
random.seed(42)
tf.random.set_seed(42)
np.random.seed(42)

def create_model(name, input_shape):
    if name == 'simple_nn':
        model = keras.Sequential([
            layers.Dense(64, activation='relu', input_shape=(input_shape,)),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            layers.Dense(3, activation='softmax')
        ])
    elif name == 'deep_nn':
        model = keras.Sequential([
            layers.Dense(128, activation='relu', input_shape=(input_shape,)),
            layers.BatchNormalization(),
            layers.Dropout(0.4),
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            layers.Dense(3, activation='softmax')
        ])
    else:  # Default model
        model = keras.Sequential([
            layers.Dense(32, activation='relu', input_shape=(input_shape,)),
            layers.Dense(16, activation='relu'),
            layers.Dense(3, activation='softmax')
        ])

    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Define models
models = {
    'Simple Neural Network': create_model('simple_nn', X_train_scaled.shape[1]),
    'Deep Neural Network': create_model('deep_nn', X_train_scaled.shape[1])
}

# Store results
results = []

# Training and evaluation
for name, model in models.items():
    print(f"\nTraining {name}...")

    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    print(model.summary())

    history = model.fit(
        X_train_scaled, y_train_cat,
        epochs=100,
        batch_size=16,
        validation_split=0.2,
        callbacks=[early_stopping],
        verbose=1
    )

    # Evaluate model
    test_loss, test_acc = model.evaluate(X_test_scaled, y_test_cat, verbose=0)

    # Get predictions
    y_pred_prob = model.predict(X_test_scaled)
    y_pred = np.argmax(y_pred_prob, axis=1)

    # Calculate metrics
    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)

    # Store results
    results.append({
        'Model': name,
        'Accuracy': test_acc,
        'Precision': report['weighted avg']['precision'],
        'Recall': report['weighted avg']['recall'],
        'F1-Score': report['weighted avg']['f1-score']
    })

# Create comparison DataFrame
evaluation_df = pd.DataFrame(results)
print("\nModel Performance Comparison:")
print(evaluation_df)

# Plot training history for each model
for name, model in models.items():
    plt.figure(figsize=(12, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title(f'{name} - Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title(f'{name} - Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.tight_layout()
    plt.show()

    # Confusion matrix
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Normal', 'Sleep Apnea', 'Insomnia'],
                yticklabels=['Normal', 'Sleep Apnea', 'Insomnia'])
    plt.title(f'Confusion Matrix - {name}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

# ----------------------------------
# Feature Importance Analysis using a simpler model
# ----------------------------------
# We'll use a simple model with L1 regularization to estimate feature importance

print("\nAnalyzing feature importance...")
feature_importance_model = keras.Sequential([
    layers.Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],),
                kernel_regularizer=keras.regularizers.l1(0.01)),
    layers.Dense(3, activation='softmax')
])

feature_importance_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

feature_importance_model.fit(
    X_train_scaled, y_train_cat,
    epochs=50,
    batch_size=16,
    verbose=0
)

# Get the weights from the first layer
weights = feature_importance_model.layers[0].get_weights()[0]
importance = np.sum(np.abs(weights), axis=1)

# Create feature importance DataFrame
feature_names = x.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
importance_df = importance_df.sort_values('Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance Analysis')
plt.tight_layout()
plt.show()

print("\nFeature Importance Ranking:")
print(importance_df)

best_model_name = evaluation_df.loc[evaluation_df['Accuracy'].idxmax(), 'Model']
best_model = models[best_model_name]

print(f"\nSaving the best model: {best_model_name}")
best_model.save('sleep_disorder_prediction_model.h5')
print("Model saved as 'sleep_disorder_prediction_model.h5'")

nnprint("\nExample prediction with the best model:")
# Create a sample input
sample = X_test.iloc[0:1]
sample_scaled = scaler.transform(sample)

# Make prediction
prediction_prob = best_model.predict(sample_scaled)
prediction_class = np.argmax(prediction_prob, axis=1)[0]

# Map prediction to class name
disorder_map = {0: 'Normal', 1: 'Sleep Apnea', 2: 'Insomnia'}
predicted_disorder = disorder_map[prediction_class]

print("Sample input features:", sample.values)
print("Prediction probabilities:", prediction_prob[0])
print(f"Predicted sleep disorder: {predicted_disorder} (class {prediction_class})")
print(f"Actual sleep disorder class: {y_test.iloc[0]} ({disorder_map[y_test.iloc[0]]})")

import tensorflowjs as tfjs
#TFJS
tfjs_target_dir = 'tfjs_model/'
tfjs.converters.save_keras_model(model, tfjs_target_dir)
print(f"> TFJS model tersimpan di folder: {tfjs_target_dir}/")

import shutil
shutil.make_archive('saved_model_eye', 'zip', 'saved_model_eye')

import shutil
shutil.make_archive('tfjs_model', 'zip', 'tfjs_model')

import shutil
shutil.make_archive('tflite', 'zip', 'tflite')